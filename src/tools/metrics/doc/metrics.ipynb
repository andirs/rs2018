{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for RecSys 2018\n",
    "Implementation for validation purposes. The official rules can be found [here](https://recsys-challenge.spotify.com/rules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Precision\n",
    "R-precision is the number of retrieved relevant tracks divided by the number of known relevant tracks (i.e., the number of withheld tracks):\n",
    "\n",
    "$$ \\text{R-precision} = \\frac{∣G \\bigcap R_{1:|G|}∣}{|G|} $$\n",
    "The metric is averaged across all playlists in the challenge set. This metric rewards total number of retrieved relevant tracks (regardless of order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = ['1', '2', '3', '5', '8', '99']\n",
    "prediction = ['5', '8', '13', '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_precision(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    R-precision is the number of retrieved relevant tracks divided by the number of known relevant tracks.\n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    relevant_tracks: list of all relevant tracks in order of appearance in prediction set\n",
    "    r-precision metric: float measure of r-precision\n",
    "    \"\"\"\n",
    "    relevant_tracks = []\n",
    "    for idx, track in enumerate(prediction):\n",
    "        if track in ground_truth and idx < len(ground_truth):\n",
    "            relevant_tracks.append(track)\n",
    "    return relevant_tracks, (len(relevant_tracks) / len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['5', '8', '3'], 0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_precision(ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_two = ['5', '8', '13', '3', '99', '87', '2', '150']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['5', '8', '3', '99'], 0.6666666666666666)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_precision(ground_truth, prediction_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_precision_two(ground_truth, prediction):\n",
    "    rel_set = set(prediction[:len(ground_truth)]).intersection(set(ground_truth))\n",
    "    return rel_set, len(rel_set) / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'3', '5', '8'}, 0.5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_precision_two(ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'3', '5', '8', '99'}, 0.6666666666666666)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_precision_two(ground_truth, prediction_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized discounted cumulative gain (NDCG)\n",
    "Discounted cumulative gain (DCG) measures the ranking quality of the recommended tracks, increasing when relevant tracks are placed higher in the list. Normalized DCG (NDCG) is determined by calculating the DCG and dividing it by the ideal DCG in which the recommended tracks are perfectly ranked:\n",
    "\n",
    "$$ DCG = rel_{1} + \\sum_{i=2}^{|R|} \\frac{rel_{i}}{log_{2}i} $$\n",
    "\n",
    "The ideal DCG or IDCG is, on our case, equal to:\n",
    "\n",
    "$$ IDCG = 1 + \\sum_{i=2}^{|G \\bigcap R|} \\frac{1}{log_{2}i}$$ \n",
    "\n",
    "If the size of the set intersection of G and R, is empty, then the DCG is equal to 0. The NDCG metric is now calculated as:\n",
    "\n",
    "$$ NDCG = \\frac{DCG}{IDCG} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance(ground_truth, item):\n",
    "    \"\"\"\n",
    "    Returns relevance measure for playlist predictions.\n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    relevance: 1 if track is in ground_truth, 0 otherwise\n",
    "    \"\"\"\n",
    "    if item in ground_truth:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Discounted cumulative gain (DCG) measures the ranking quality of the recommended tracks.\n",
    "    DCG increases when relevant tracks are placed higher in the list. \n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    relevance: float representing the relevance metric for a given playlist prediction\n",
    "    \"\"\"\n",
    "    relevance = None\n",
    "    for idx, track in enumerate(prediction):\n",
    "        if not relevance:\n",
    "            relevance = get_relevance(ground_truth, track)\n",
    "        else:\n",
    "            relevance += get_relevance(ground_truth, track) / np.log2(idx + 1)\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idcg(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Maximum measure for a prediction set\n",
    "    if all predictions were relevant.\n",
    "    \"\"\"\n",
    "    relevance = None\n",
    "    idx = 0\n",
    "    for track in prediction:\n",
    "        idx += 1\n",
    "        if not relevance:\n",
    "            relevance = 1\n",
    "        else:\n",
    "            relevance += 1 / np.log2(idx)\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1', '2', '3', '5', '8', '99'], ['5', '8', '13', '3'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.5\n",
    "dcg(ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1309297535714578"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1309...\n",
    "idcg(ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Normalized discounted cumulative gain (NDCG). \n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    NDCG: float - discounted cumulative gain given the ideal discounted cumulative gain\n",
    "    \n",
    "    \"\"\"\n",
    "    return dcg(ground_truth, prediction) / idcg(ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7984848580994974"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.7984...\n",
    "ndcg(ground_truth, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Song Clicks\n",
    "Recommended Songs is a Spotify feature that, given a set of tracks in a playlist, recommends 10 tracks to add to the playlist. The list can be refreshed to produce 10 more tracks. Recommended Songs clicks is the number of refreshes needed before a relevant track is encountered. It is calculated as follows:\n",
    "\n",
    "$$ \\text{clicks} = \t\\lfloor \\frac{\\text{arg min}_{i} \\{R_{i}: R_{i} \\in G \\} - 1}{10} \\rfloor$$\n",
    "\n",
    "If the metric does not exist (i.e. if there is no relevant track in (R), a value of 51 is picked (which is 1 + the maximum number of clicks possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsc(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Recommended Songs is a Spotify feature that, given a set of tracks in a playlist, \n",
    "    recommends 10 tracks to add to the playlist. \n",
    "    The list can be refreshed to produce 10 more tracks. \n",
    "    Recommended Songs clicks is the number of refreshes \n",
    "    needed before a relevant track is encountered\n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    counter: amount of clicks needed for the first relevant song to appear\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    for idx, track in enumerate(prediction):\n",
    "        if idx % 10 == 0:\n",
    "            counter += 1\n",
    "        if track in ground_truth:\n",
    "            return counter\n",
    "    return counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_rsc_one = [1]\n",
    "ground_truth_rsc_two = [499]\n",
    "ground_truth_rsc_three = [500]\n",
    "prediction_rsc_one = range(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "rsc(ground_truth_rsc_one, prediction_rsc_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50\n",
    "rsc(ground_truth_rsc_two, prediction_rsc_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 51\n",
    "rsc(ground_truth_rsc_three, prediction_rsc_one)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
